{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Regresión Logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos que tenemos un conjunto de datos $\\mathcal{D} = \\{(x_i, y_i) \\in \\mathbb{R}^p \\times \\{0,1\\}: i \\in[m]\\}$. El método de *regresión logística* asume que $Pr[y_i|x_i,\\beta] \\sim Bernoulli(\\mu_i)$. Queremos encontrar el modelo $\\hat{\\beta} \\in \\mathbb{R}^p$ que mejor se ajuste a $\\mathcal{D}$. Si encontramos $\\hat{\\beta}$ podemos usarlo para modelar $Pr[y|\\boldsymbol{x},\\hat{\\beta}]$ y predecir la etiqueta $\\hat{y} \\in \\{0,1\\}$ de un nueva dato $\\boldsymbol{x}$.\n",
    "\n",
    "La *log-verosimilitud negativa* está dada por\n",
    "$$ F(\\beta) := LVN(\\beta) = - \\sum_{i=1}^m [y_i \\log \\mu_i + (1-y_i) \\log (1-\\mu_i)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Escriba las expresiones para $\\nabla F(\\beta)$ y $\\nabla^2 F(\\beta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla F(\\beta) = X^T(\\mu-y)$$\n",
    "$$\\nabla^2 F(\\beta) = X^TSX$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use estas expresiones para implementar `grad_F` y `hess_F` en Python. Ambas funciones tienen como argumento `(X,y,beta)` con `X.shape=(m,p)`, `y.shape=(m,)` y `beta.shape=(p,)` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero definimos la función *sigmoide* que se usará en ambas fuciones, así como las bibliotecas que se usarán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def sigm(X):\n",
    "  return 1/(1+np.exp(-X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posteriormente definimos el gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_F(X,y,beta):\n",
    "  m = sigm(X@beta) \n",
    "  return X.T@(m-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De igual manera, definimos el hessiano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hess_F(X,y,beta): \n",
    "  m = sigm(X@beta) \n",
    "  S = np.outer(m.T,(1-m)) \n",
    "  return (X.T@S)@X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Implemente el método de *máximo descenso* para minimizar $F(\\beta)$. Elija un $\\beta^0$ aleatorio y una tolerancia de $\\epsilon = 10^{-8}$. Encuentre $a^k$ usando la *condición de Armijo* (vista en clase). Usar $\\tau = 0.5$ y $\\gamma = 0.1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero definimos la función de *log-verosimilitud negativa*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LVN(X,y,beta):\n",
    "  m = sigm(X@beta) \n",
    "  return -(y@np.log(m) + (1-y)@np.log(1-m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuacion se define la fución del método de *máximo descenso* usandoo *Armijo*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_desc_armijo(X,y):\n",
    "  \n",
    "  #Definimos los parámetros\n",
    "  m,p = X.shape\n",
    "  beta = np.zeros(p)# np.random.uniform(size = p)\n",
    "  tau = 0.5\n",
    "  gamma = 0.1\n",
    "  epsilon = 10**-8 \n",
    "  alfa = 0.00001 \n",
    "  niter = 0\n",
    "  \n",
    "  #Estos son los valores iniciales\n",
    "  p = -grad_F(X,y,beta)\n",
    "  leftf = LVN(X,y,beta + alfa*p)\n",
    "  rightf = LVN(X,y,beta) + gamma*alfa*(grad_F(X,y,beta)@p)\n",
    "  \n",
    "  #Iteraciones hasta que se compla la condición\n",
    "  while leftf + epsilon < rightf:\n",
    "    niter += 1\n",
    "    alfa = alfa*tau\n",
    "    beta = beta + alfa*p\n",
    "    p = -grad_F(X,y,beta)\n",
    "    leftf = LVN(X,y,beta + alfa*p)\n",
    "    rightf = LVN(X,y,beta) + gamma*alfa*(grad_F(X,y,beta)@p)\n",
    "  \n",
    "  return beta, niter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Implemente un clasificador de regresión logística para obtener $\\hat{y}$. En Python `yhat = pred(x, betahat)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función de predicción es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(x, betahat):\n",
    "  return((sigm(x@betahat)>0.5)*1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Implemente la siguiente función de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datos(modo='entrena'):\n",
    "  gid = 'd932a3cf4d6bdeef36a7230fff959301'\n",
    "  tail = '64b604aedff376b7757b533d1c93685ce19b2077/bcdata'\n",
    "  url = 'https://gist.githubusercontent.com/rodrgo/%s/raw/%s' % (gid, tail)\n",
    "  df = pd.read_csv(url, sep=',')\n",
    "  df = df.drop(columns=['Unnamed: 32', 'id'])\n",
    "  var = 'diagnosis'\n",
    "  df.loc[df[var] == 'M', [var]] = 1\n",
    "  df.loc[df[var] == 'B', [var]] = 0\n",
    "  X_cols = [c for c in df.columns if not c is var]\n",
    "  X, y = df[X_cols].to_numpy(), df[var].to_numpy()\n",
    "  idx = np.random.permutation(X.shape[0])\n",
    "  train_idx, test_idx = idx[:69], idx[69:]\n",
    "  idx = train_idx if modo == 'entrena' else test_idx\n",
    "  return X[idx,:], y[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cree dos conjuntos de datos:<br>\n",
    "`X_entrena, y_entrena = datos()`<br>\n",
    "`X_prueba, y_prueba = datos(modo='prueba')`<br>\n",
    "Use `(X_entrena, y_entrena)` para ajusar un valor $\\hat{\\beta}$(`betahat`) usando máximo descenso. Use la función `pred`\n",
    "para predecir la etiqueta de cada punto en el conjunto de datos `X_prueba`. Compare esta respuesta con `y_prueba` para obtener el error de predicción de su modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero creamos el conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_entrena, y_entrena = datos()\n",
    "X_prueba, y_prueba = datos(modo='prueba')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación usaremos haremos la predicción de los datos con las funciones anteriores. Primero se normalizan los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "Xst_entrena = scaler.fit_transform(X_entrena)\n",
    "Xst_prueba = scaler.transform(X_prueba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora entrenamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "betahat_armijo, i = grad_desc_armijo(Xst_entrena,y_entrena)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con $\\hat{\\beta}$ haremos la predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = pred(Xst_prueba, betahat_armijo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación calculamos el error de predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.628"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(yhat-y_prueba)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Ahora implemente en Python el método de Newton para minimizar $F(\\beta)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_desc_newton(X,y):\n",
    "  \n",
    "  #Definimos los parámetros\n",
    "  m,p = X.shape\n",
    "  beta = np.random.uniform(size = p)\n",
    "  tau = 0.5\n",
    "  gamma = 0.1\n",
    "  epsilon = 10**-8 \n",
    "  alfa = 0.000000000000001 \n",
    "  niter = 0\n",
    "  \n",
    "  #Estos son los valores iniciales\n",
    "  p = -np.linalg.inv(hess_F(X,y,beta))@grad_F(X,y,beta)\n",
    "  leftf = LVN(X,y,beta + alfa*p)\n",
    "  rightf = LVN(X,y,beta) + gamma*alfa*(grad_F(X,y,beta)@p)\n",
    "  \n",
    "  #Iteraciones hasta que se compla la condición\n",
    "  while leftf + epsilon < rightf:\n",
    "    niter += 1\n",
    "    alfa = alfa*tau\n",
    "    beta = beta + alfa*p\n",
    "    p = -grad_F(X,y,beta)\n",
    "    leftf = LVN(X,y,beta + alfa*p)\n",
    "    rightf = LVN(X,y,beta) + gamma*alfa*(grad_F(X,y,beta)@p)\n",
    "    print(LVN(X,y,beta))\n",
    "  \n",
    "  return beta, niter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
